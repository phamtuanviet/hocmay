{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-10T03:06:50.176143Z",
     "iopub.status.busy": "2025-08-10T03:06:50.175760Z",
     "iopub.status.idle": "2025-08-10T03:06:50.183469Z",
     "shell.execute_reply": "2025-08-10T03:06:50.182456Z",
     "shell.execute_reply.started": "2025-08-10T03:06:50.176114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "from functools import partial\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from astropy.stats import sigma_clip\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.optimize import minimize\n",
    "from scipy import optimize\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:06:50.185469Z",
     "iopub.status.busy": "2025-08-10T03:06:50.185148Z",
     "iopub.status.idle": "2025-08-10T03:06:50.203478Z",
     "shell.execute_reply": "2025-08-10T03:06:50.202748Z",
     "shell.execute_reply.started": "2025-08-10T03:06:50.185440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GlobalVariable:\n",
    "    def __init__(self, base_path='/kaggle/input/ariel-data-challenge-2025'):\n",
    "        self.base_path = base_path\n",
    "        self.adc_info = pd.read_csv(f'{base_path}/adc_info.csv')\n",
    "        self.axis_info = pd.read_parquet(f'{base_path}/axis_info.parquet')\n",
    "        self.train_labels = pd.read_csv(f'{base_path}/train.csv', index_col='planet_id')\n",
    "        self.train_info = pd.read_csv(f'{base_path}/train_star_info.csv', index_col='planet_id')\n",
    "        self.test_info = pd.read_csv(f'{base_path}/test_star_info.csv', index_col='planet_id')\n",
    "        self.sample_test = pd.read_csv(f'{base_path}/sample_submission.csv', index_col='planet_id')\n",
    "        self.dataset = 'train'\n",
    "        self.cut_inf ,self.cut_sup = 39, 321\n",
    "        self.nums_parallel = 4\n",
    "\n",
    "        gainAIRS = self.adc_info['AIRS-CH0_adc_gain'].iloc[0]\n",
    "        gainFGS1 = self.adc_info['FGS1_adc_gain'].iloc[0]\n",
    "        offsetAIRS = self.adc_info['AIRS-CH0_adc_offset'].iloc[0]\n",
    "        offsetFGS1 = self.adc_info['FGS1_adc_offset'].iloc[0]\n",
    "        self.adc = {\n",
    "            'AIRS-CH0': {'gain': gainAIRS, 'offset': offsetAIRS},\n",
    "            'FGS1': {'gain': gainFGS1, 'offset': offsetFGS1}\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrating and binning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:06:50.204882Z",
     "iopub.status.busy": "2025-08-10T03:06:50.204551Z",
     "iopub.status.idle": "2025-08-10T03:06:50.229115Z",
     "shell.execute_reply": "2025-08-10T03:06:50.228128Z",
     "shell.execute_reply.started": "2025-08-10T03:06:50.204861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tham khảo từ https://www.kaggle.com/code/gordonyip/calibrating-and-binning-ariel-data\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    # Sửa sai tuyến tính data\n",
    "    def apply_linear_corr(self, linear_corr,clean_signal):\n",
    "        linear_corr = np.flip(linear_corr, axis=0)\n",
    "        for x, y in itertools.product(\n",
    "                    range(clean_signal.shape[1]), range(clean_signal.shape[2])\n",
    "                ):\n",
    "            poli = np.poly1d(linear_corr[:, x, y])\n",
    "            clean_signal[:, x, y] = poli(clean_signal[:, x, y])\n",
    "        return clean_signal\n",
    "\n",
    "    # trừ dark current\n",
    "    def clean_dark(self, signal, dark, dt):\n",
    "        dark = np.tile(dark, (signal.shape[0], 1, 1))\n",
    "        signal -= dark* dt[:, np.newaxis, np.newaxis]\n",
    "        return signal\n",
    "\n",
    "    # tiền xử lý dữ liệu đầu vào và binning\n",
    "    def preproc(self, dataset, sensor, binning = 15):\n",
    "        \n",
    "        sensor_sizes_dict = {\"AIRS-CH0\":[[11250, 32, 356], [1, 32, 356]], \"FGS1\":[[135000, 32, 32], [1, 32, 32]]}\n",
    "        binned_dict = {\"AIRS-CH0\":[11250 // binning // 2, 356], \"FGS1\":[135000 // binning // 2]}\n",
    "        linear_corr_dict = {\"AIRS-CH0\":(6, 32, 356), \"FGS1\":(6, 32, 32)}\n",
    "        planet_ids = self.config.train_labels.index\n",
    "\n",
    "        # phân biệt dang là dữ liệu train hay test\n",
    "        if dataset == \"test\":\n",
    "            sample_sub_path = f\"{self.config.base_path}/sample_submission.csv\"\n",
    "            planet_ids = pd.read_csv(sample_sub_path)[\"planet_id\"].tolist()\n",
    "        \n",
    "        feats = []\n",
    "\n",
    "        # đọc qua từng hành tinh các thông số của nó và xử lý\n",
    "        for i, planet_id in tqdm(list(enumerate(planet_ids))):\n",
    "            signal = pd.read_parquet(f'{self.config.base_path}/{dataset}/{planet_id}/{sensor}_signal_0.parquet').to_numpy()\n",
    "            dark_frame = pd.read_parquet(f'{self.config.base_path}/{dataset}/' + str(planet_id) + '/' + sensor + '_calibration_0/dark.parquet', engine='pyarrow').to_numpy()\n",
    "            dead_frame = pd.read_parquet(f'{self.config.base_path}/{dataset}/' + str(planet_id) + '/' + sensor + '_calibration_0/dead.parquet', engine='pyarrow').to_numpy()\n",
    "            flat_frame = pd.read_parquet(f'{self.config.base_path}/{dataset}/' + str(planet_id) + '/' + sensor + '_calibration_0/flat.parquet', engine='pyarrow').to_numpy()\n",
    "            linear_corr = pd.read_parquet(f'{self.config.base_path}/{dataset}/' + str(planet_id) + '/' + sensor + '_calibration_0/linear_corr.parquet').values.astype(np.float64).reshape(linear_corr_dict[sensor])\n",
    "\n",
    "            # chuyển đổi tín hiệu về pixcel voltage\n",
    "            signal = signal.reshape(sensor_sizes_dict[sensor][0]) \n",
    "            gain = self.config.adc[sensor]['gain']\n",
    "            offset = self.config.adc[sensor]['offset']\n",
    "            signal = signal / gain + offset\n",
    "\n",
    "            # phát hiển pixcel hot\n",
    "            hot = sigma_clip(\n",
    "                dark_frame, sigma=5, maxiters=5\n",
    "            ).mask\n",
    "\n",
    "            # xử lý với các loại tín hiệu\n",
    "            if sensor != \"FGS1\":\n",
    "                dt = np.ones(len(signal))*0.1 \n",
    "                dt[1::2] += 4.5\n",
    "            else:\n",
    "                dt = np.ones(len(signal))*0.1\n",
    "                dt[1::2] += 0.1\n",
    "\n",
    "\n",
    "            # gán 0 cho những phần từ nhỏ hơn 0\n",
    "            signal = signal.clip(0)\n",
    "            linear_corr_signal = self.apply_linear_corr(linear_corr, signal)\n",
    "            signal = self.clean_dark(linear_corr_signal, dark_frame, dt)\n",
    "\n",
    "            # chia dư liệu cho flat (độ lệch giữa các pixcel gốc)\n",
    "            flat = flat_frame.reshape(sensor_sizes_dict[sensor][1])\n",
    "            flat[dead_frame.reshape(sensor_sizes_dict[sensor][1])] = np.nan\n",
    "            flat[hot.reshape(sensor_sizes_dict[sensor][1])] = np.nan\n",
    "            signal = signal / flat\n",
    "            \n",
    "            # cắt ở trung tâm thu gọn dữ liệu lấy những dữ liệu chuẩn nhất phù hợp với bước sóng thứ nhất test\n",
    "            if sensor == \"FGS1\":\n",
    "                signal = signal[:,10:22,10:22] \n",
    "                signal = signal.reshape(sensor_sizes_dict[sensor][0][0],144) \n",
    "\n",
    "            # cắt ở trung tâm thu gọn dữ liệu lấy những dữ liệu chuẩn nhất\n",
    "            if sensor != \"FGS1\":\n",
    "                signal = signal[:,10:22,:]\n",
    "\n",
    "            # trừ 2 pixcel chẵn lẻ\n",
    "            mean_signal = np.nanmean(signal, axis=1) \n",
    "            cds_signal = (mean_signal[1::2] - mean_signal[0::2])\n",
    "\n",
    "            # binning thu gọn chắt lọc dữ liệu\n",
    "            binned = np.zeros((binned_dict[sensor]))\n",
    "            for j in range(cds_signal.shape[0] // binning):\n",
    "                binned[j] = cds_signal[j*binning:j*binning+binning].mean(axis=0) \n",
    "                       \n",
    "            if sensor == \"FGS1\":\n",
    "                binned = binned.reshape((binned.shape[0],1))\n",
    "            \n",
    "            feats.append(binned)\n",
    "            \n",
    "        return np.stack(feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:06:50.295347Z",
     "iopub.status.busy": "2025-08-10T03:06:50.295040Z",
     "iopub.status.idle": "2025-08-10T03:06:50.319871Z",
     "shell.execute_reply": "2025-08-10T03:06:50.318913Z",
     "shell.execute_reply.started": "2025-08-10T03:06:50.295325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# xử lý dữ liệu\n",
    "# Tham khảo từ https://www.kaggle.com/code/vitalykudelya/neurips-non-ml-transit-curve-fitting?scriptVersionId=250312271\n",
    "# và từ https://www.kaggle.com/code/jiazhuang/adc2024-fix-window-weights-finetuning/notebook\n",
    "class DataProcessing:\n",
    "    def __init__ (self, config):\n",
    "        self.config = config\n",
    "        self.data_pre = DataPreprocessing(config)\n",
    "\n",
    "        # đọc lấy các file nếu đã xử lý trước đó\n",
    "        if os.path.exists('/kaggle/input/data03/f_raw_train.npy'):\n",
    "            self.f_raw_train = np.load('/kaggle/input/data03/f_raw_train.npy')\n",
    "            print(\"Loading f done\")\n",
    "        else :\n",
    "            self.f_raw_train = self.data_pre.preproc(f'{self.config.dataset}',\"FGS1\", 30*12)\n",
    "            self.f_raw_train.shape\n",
    "            np.save('/kaggle/working/f_raw_train.npy', self.f_raw_train)\n",
    "        \n",
    "        if os.path.exists('/kaggle/input/data03/a_raw_train.npy'):\n",
    "            self.a_raw_train = np.load('/kaggle/input/data03/a_raw_train.npy')\n",
    "            print(\"Loading a done\")\n",
    "        else :\n",
    "            self.a_raw_train = self.data_pre.preproc(f'{self.config.dataset}',\"AIRS-CH0\", 30)\n",
    "            self.a_raw_train.shape\n",
    "            np.save('/kaggle/working/a_raw_train.npy', self.a_raw_train)\n",
    "\n",
    "        # đọc dữ liệu cần test\n",
    "        self.f_raw_test = self.data_pre.preproc('test',\"FGS1\", 30*12)\n",
    "        self.a_raw_test = self.data_pre.preproc('test',\"AIRS-CH0\", 30)\n",
    "\n",
    "    # làm mượt tín hiệu AIRS và cắt lấy những bước sóng ở giữa \n",
    "    # mượt bằng cách lấy chung bình 37 bước trái phải\n",
    "    def smooth_airs(self,a_raw_data, window = 37):\n",
    "        wv_smooth_a_raw = []\n",
    "        for i in range(self.config.cut_inf, self.config.cut_sup):\n",
    "           wv_smooth_a_raw.append(a_raw_data[:, :, max(i-window, 0):(i+window)].mean(axis=-1))\n",
    "    \n",
    "        wv_smooth_a_raw = np.stack(wv_smooth_a_raw, axis=-1)\n",
    "        wv_smooth_a_raw.shape\n",
    "\n",
    "        return wv_smooth_a_raw\n",
    "\n",
    "    # function tìm ra điểm transit của tín hiệu gốc bằng cách tính đạo hàm của các điểm trên signal\n",
    "    def phase_detector(self, signal):\n",
    "        \n",
    "        MIN = np.argmin(signal[30:140])+30\n",
    "        signal1 = signal[:MIN ]\n",
    "        signal2 = signal[MIN :]\n",
    "    \n",
    "        first_derivative1 = np.gradient(signal1)\n",
    "        first_derivative1 /= first_derivative1.max()\n",
    "        first_derivative2 = np.gradient(signal2)\n",
    "        first_derivative2 /= first_derivative2.max()\n",
    "    \n",
    "        phase1 = np.argmin(first_derivative1)\n",
    "        phase2 = np.argmax(first_derivative2) + MIN\n",
    "    \n",
    "        return phase1, phase2\n",
    "\n",
    "    # function tìm giá trị lệch nhỏ nhất của tín hiệu cho vào sau khi * s ở transit với giá trị của một đa thức bậc nhỏ hơn 4 khớp nhất với tín hiệu\n",
    "    # mục tiêu là minimal q tìm ra giá trị s phù hợp nhất\n",
    "    def objective(self, signal, p1, p2, s):\n",
    "        \n",
    "        best_q = 1e10\n",
    "        n = signal.shape[0]\n",
    "        try:\n",
    "            for i in range(4) :\n",
    "                delta = 2\n",
    "                end_before   = max(p1 - delta, 0) \n",
    "                start_after   = min(p2 + delta, n)\n",
    "                y = signal[:end_before].tolist() + (signal[p1+delta:p2 - delta] * (1 + s)).tolist() + signal[start_after:].tolist()\n",
    "                x = list(range(len(y)))\n",
    "                z = np.polyfit(x, y, deg=i)\n",
    "                p = np.poly1d(z)\n",
    "                q = np.abs(p(x) - y).mean()\n",
    "        \n",
    "            if q < best_q :\n",
    "                best_q = q\n",
    "        \n",
    "            return q\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR objective()] {signal.shape} p1={p1}, p2={p2} x={len(x)}, y={len(y)}, p1={p1}, p2={p2}, s={s}, err={e}\")\n",
    "            return 1e10\n",
    "\n",
    "    # tìm s bằng Nelder-Mead tự điều chỉnh sao cho q min\n",
    "    def fit_transit_depth(self, signal, phase_signal):\n",
    "        p1, p2 = self.phase_detector(phase_signal)\n",
    "        f = partial(self.objective, signal, p1, p2)\n",
    "        r = minimize(f, [0.0001], method= 'Nelder-Mead')\n",
    "        s = r.x[0]\n",
    "        return s\n",
    "\n",
    "    # tìm giá trị s cho từng bước sóng\n",
    "    def fit_every_wv(self, a_raw, phase_signal):\n",
    "        res = []\n",
    "        for i in range(282):\n",
    "            signal = a_raw[:, i]\n",
    "            res.append(\n",
    "                self.fit_transit_depth(signal, phase_signal)\n",
    "            )\n",
    "        return np.array(res, dtype=np.float32).clip(0)\n",
    "\n",
    "    # làm mượt dữ liệu bằng đa thức bậc 3 và nội suy trong khoảng +- windowsize/2\n",
    "    def smooth_data(self, data, window_size):\n",
    "        return savgol_filter(data, window_size, 3)\n",
    "\n",
    "    # xử lý dữ liệu\n",
    "    def process_data(self, a_raw_data, f_raw_data, type_data = 'train'):\n",
    "\n",
    "        # tổng số dòng\n",
    "        if type_data == 'train':\n",
    "            total_rows = self.config.train_labels.shape[0]\n",
    "        else:\n",
    "            total_rows =len(self.config.sample_test.index)\n",
    "\n",
    "        # làm mượt AIRS\n",
    "        wv_smooth_a_raw = self.smooth_airs(a_raw_data)\n",
    "\n",
    "        # cắt tín hiệu\n",
    "        a_raw_mean = a_raw_data[:, :, self.config.cut_inf:self.config.cut_sup].mean(axis=-1)\n",
    "        a_raw_mean.shape\n",
    "\n",
    "        # tính toán một độ sâu duy nhất của FGS1 (wl dự đoán chỉ có duy nhất 1 bước sóng thuộc FGS1)\n",
    "        with ThreadPoolExecutor(max_workers=self.config.nums_parallel) as exe:\n",
    "            f_depth = list( tqdm(exe.map(self.fit_transit_depth, f_raw_data[:, :, 0], a_raw_mean), total=total_rows) )\n",
    "        f_depth = np.array(f_depth, dtype=np.float32)\n",
    "\n",
    "        # đọc xem đã có file độ sâu của 282 bước sóng chưa\n",
    "        if os.path.exists('/kaggle/input/data03/a_wv_depth.npy') and type_data == 'train': \n",
    "            a_wv_depth = np.load('/kaggle/input/data03/a_wv_depth.npy')\n",
    "        else :\n",
    "            a_wv_depth = Parallel(\n",
    "                n_jobs=self.config.nums_parallel,\n",
    "                backend='loky',\n",
    "                prefer='processes'\n",
    "            )(\n",
    "                delayed(self.fit_every_wv)(ws, m)\n",
    "                for ws, m in tqdm(zip(wv_smooth_a_raw, a_raw_mean),\n",
    "                                   total=total_rows)\n",
    "            )\n",
    "            a_wv_depth = np.vstack(a_wv_depth)\n",
    "            if type_data == 'train':\n",
    "                np.save('/kaggle/working/a_wv_depth.npy', a_wv_depth)\n",
    "\n",
    "\n",
    "        # làm mượt độ sâu với hàm bậc 3 và nội suy\n",
    "        window_size = 103\n",
    "        smoothed = []\n",
    "        for pred in tqdm(a_wv_depth):\n",
    "            smooth_pred = self.smooth_data(pred, window_size=window_size)\n",
    "            smoothed.append(smooth_pred)\n",
    "        smoothed = np.row_stack(smoothed)\n",
    "        smoothed.shape\n",
    "            \n",
    "        a_wv_depth = smoothed\n",
    "\n",
    "        window = 64\n",
    "        tail_a_raw = a_raw_data[:,:, -window:].mean(axis=-1)\n",
    "        with ThreadPoolExecutor(max_workers=self.config.nums_parallel) as exe:\n",
    "            tail_a_depth = list( tqdm(exe.map(self.fit_transit_depth, tail_a_raw, a_raw_mean), total=total_rows) )\n",
    "\n",
    "        tail_a_depth = np.array(tail_a_depth, dtype=np.float32)\n",
    "        merged_f_depth = tail_a_depth * 0.9 + f_depth * 0.1\n",
    "\n",
    "\n",
    "        # chuẩn hóa dữ liệu meta data của các hành tinh\n",
    "        scaler_meta = StandardScaler()\n",
    "            \n",
    "        if type_data == 'train':\n",
    "            meta_features = scaler_meta.fit_transform(self.config.train_info.values)\n",
    "        else:\n",
    "            meta_features = scaler_meta.fit_transform(self.config.test_info.values)\n",
    "\n",
    "        wv_smooth_pred = np.column_stack([\n",
    "            merged_f_depth,\n",
    "            a_wv_depth[:, ::-1]\n",
    "        ])\n",
    "        wv_smooth_pred.shape\n",
    "\n",
    "        # ghép dữ liệu meta data với dữ liệu đô sâu s\n",
    "        X = np.hstack([wv_smooth_pred, meta_features])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:06:50.321485Z",
     "iopub.status.busy": "2025-08-10T03:06:50.321227Z",
     "iopub.status.idle": "2025-08-10T03:06:50.344995Z",
     "shell.execute_reply": "2025-08-10T03:06:50.343929Z",
     "shell.execute_reply.started": "2025-08-10T03:06:50.321457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Xây dựng model\n",
    "class DevelopedModel:\n",
    "    def __init__(self,\n",
    "                 mu_model_path=\"/kaggle/input/model_001/scikitlearn/default/3/mu_model.pkl\",\n",
    "                 sigma_model_path=\"/kaggle/input/model_001/scikitlearn/default/3/sigma_model.pkl\"):\n",
    "\n",
    "        self.mu_model_path = mu_model_path\n",
    "        self.sigma_model_path = sigma_model_path\n",
    "        self.mu_model = None\n",
    "        self.sigma_model = None\n",
    "        self.trained = False\n",
    "\n",
    "        if os.path.exists(self.mu_model_path) and os.path.exists(self.sigma_model_path):\n",
    "            print(\"Loading existing models...\")\n",
    "            self.mu_model = joblib.load(self.mu_model_path)\n",
    "            self.sigma_model = joblib.load(self.sigma_model_path)\n",
    "\n",
    "    # traning model với tập dữ liệu X và đầu ra y với test mặc định là 10%\n",
    "    def fit(self, X, y, train_size=0):\n",
    "        if self.mu_model is not None and self.sigma_model is not None:\n",
    "            print(\"Models already loaded. Skipping training.\")\n",
    "            return\n",
    "\n",
    "        # self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "        #     X, y, train_size=train_size, random_state=42\n",
    "        # )\n",
    "\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "\n",
    "        base_model_sigma = XGBRegressor(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        base_model = LinearRegression()\n",
    "        eps = 1e-6\n",
    "\n",
    "\n",
    "        print(\"Developing model ...\")\n",
    "\n",
    "        self.mu_model = MultiOutputRegressor(base_model)\n",
    "        mu_oof = cross_val_predict(self.mu_model, self.X_train, self.y_train, cv=5)\n",
    "        sigma_true = np.abs(self.y_train - mu_oof)\n",
    "\n",
    "        self.sigma_model = MultiOutputRegressor(base_model_sigma)\n",
    "        \n",
    "        self.mu_model.fit(self.X_train, self.y_train)\n",
    "        self.sigma_model.fit(self.X_train, sigma_true)\n",
    "        #     base_model = SVR(\n",
    "        #         kernel='rbf',\n",
    "        #         C=1.0,\n",
    "        #         epsilon=0.1\n",
    "        #     )\n",
    "\n",
    "        # base_model = Ridge(alpha=1.0)\n",
    "\n",
    "        # base_model = Lasso(alpha=0.1, max_iter=10000)\n",
    "\n",
    "        # Model xử dụng\n",
    "        # base_model = LinearRegression()\n",
    "        \n",
    "        print(\"Saving models to /kaggle/working/ ...\")\n",
    "        joblib.dump(self.mu_model, \"/kaggle/working/mu_model.pkl\")\n",
    "        joblib.dump(self.sigma_model, \"/kaggle/working/sigma_model.pkl\")\n",
    "\n",
    "        self.trained = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.mu_model is None or self.sigma_model is None:\n",
    "            raise RuntimeError(\"Model not trained or loaded.\")\n",
    "        mu_pred = self.mu_model.predict(X)\n",
    "        sigma_pred = self.sigma_model.predict(X)\n",
    "        return mu_pred, sigma_pred\n",
    "\n",
    "    def evaluate(self):\n",
    "        if not self.trained:\n",
    "            print(\"Model was loaded (not trained), skipping evaluation plots.\")\n",
    "            return\n",
    "    \n",
    "        mu_pred, sigma_pred = self.predict(self.X_test)\n",
    "        abs_errors = np.abs(self.y_test - mu_pred)\n",
    "        mse = mean_squared_error(self.y_test, mu_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(self.y_test, mu_pred)\n",
    "        r2 = r2_score(self.y_test, mu_pred)\n",
    "        mean_sigma = np.mean(sigma_pred)\n",
    "     \n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"R² Score: {r2:.4f}\")\n",
    "        print(f\"Mean predicted sigma (uncertainty): {mean_sigma:.4f}\")\n",
    "    \n",
    "        # Vẽ biểu đồ\n",
    "        plt.figure(figsize=(18, 5))\n",
    "    \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.scatter(self.y_test.ravel(), mu_pred.ravel(), alpha=0.3)\n",
    "        plt.xlabel(\"True\")\n",
    "        plt.ylabel(\"Predicted\")\n",
    "        plt.title(\"True vs Predicted\")\n",
    "    \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.hist(abs_errors.ravel(), bins=50, color='orange', alpha=0.8)\n",
    "        plt.xlabel(\"Absolute Error\")\n",
    "        plt.title(\"Distribution of Absolute Errors\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:06:50.346424Z",
     "iopub.status.busy": "2025-08-10T03:06:50.346064Z",
     "iopub.status.idle": "2025-08-10T03:06:50.365083Z",
     "shell.execute_reply": "2025-08-10T03:06:50.364057Z",
     "shell.execute_reply.started": "2025-08-10T03:06:50.346399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tạo submission in ra file\n",
    "class SubmissionGenerator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X_new):\n",
    "        mu, sigma = self.model.predict(X_new)\n",
    "        return mu, sigma\n",
    "\n",
    "    def save_submission(self, mu, sigma=None, filename=\"submission.csv\", sample_path=\"/kaggle/input/ariel-data-challenge-2025/sample_submission.csv\"):\n",
    "        planet_ids = pd.read_csv(sample_path)[\"planet_id\"].astype(int).tolist()\n",
    "        mu_columns = [f\"wl_{i+1}\" for i in range(283)]\n",
    "        sigma_columns = [f\"sigma_{i+1}\" for i in range(283)]\n",
    "\n",
    "        submission_df = pd.DataFrame(\n",
    "        np.hstack([mu, sigma]),\n",
    "            columns=mu_columns + sigma_columns\n",
    "        )\n",
    "        submission_df.insert(0, \"planet_id\", planet_ids)\n",
    "        submission_df[\"planet_id\"] = submission_df[\"planet_id\"].astype(int)\n",
    "        submission_df.to_csv(filename, index=False, float_format=\"%.6f\")\n",
    "        print(f\"Submission saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:06:50.367215Z",
     "iopub.status.busy": "2025-08-10T03:06:50.366957Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading f done\n",
      "Loading a done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.21s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.02s/it]\n",
      " 37%|███▋      | 405/1100 [00:35<00:59, 11.68it/s]"
     ]
    }
   ],
   "source": [
    "# Load và xử lý dữ liệu\n",
    "config = GlobalVariable()\n",
    "data_processing = DataProcessing(config)\n",
    "\n",
    "a_raw_train = data_processing.a_raw_train\n",
    "f_raw_train = data_processing.f_raw_train\n",
    "\n",
    "X_train = data_processing.process_data(a_raw_train,f_raw_train)\n",
    "y_mu  = config.train_labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Huấn luyện và lưu model\n",
    "model = DevelopedModel()\n",
    "model.fit(X_train, y_mu)\n",
    "# model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load và xử lý dữ liệu test\n",
    "a_raw_test = data_processing.a_raw_test\n",
    "f_raw_test = data_processing.f_raw_test\n",
    "\n",
    "X_test = data_processing.process_data(a_raw_test,f_raw_test,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model và tạo file submission\n",
    "submission_gen = SubmissionGenerator(model)\n",
    "mu_pred, sigma_pred = submission_gen.predict(X_test)\n",
    "submission_gen.save_submission(mu_pred, sigma_pred)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13093295,
     "isSourceIdPinned": false,
     "sourceId": 101849,
     "sourceType": "competition"
    },
    {
     "datasetId": 8017121,
     "sourceId": 12687555,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 424253,
     "modelInstanceId": 406333,
     "sourceId": 514651,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
